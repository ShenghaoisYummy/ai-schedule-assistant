import os
import torch
import pickle
import numpy as np
from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification

from config import INTENT_MODEL_PATH, LABEL_ENCODER_PATH, NER_MODEL_PATH, BERT_MODEL_NAME

class ModelManager:
    def __init__(self):
        self.models = {}
        self.models_loaded = False
        self.sklearn_models_loaded = False
        self.transformers_models_loaded = False
        self.load_models()
    
    def load_models(self):
        """åŠ è½½æ‰€æœ‰éœ€è¦çš„æ¨¡å‹"""
        print("ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹...")
        
        # Try to load sklearn models first
        self._load_sklearn_models()
        
        # Try to load transformer models
        self._load_transformer_models()
        
        if self.sklearn_models_loaded or self.transformers_models_loaded:
            self.models_loaded = True
            print("âœ… è‡³å°‘éƒ¨åˆ†æ¨¡å‹åŠ è½½æˆåŠŸï¼Œåº”ç”¨å¯ä»¥è¿è¡Œ")
        else:
            print("âš ï¸ æ‰€æœ‰æ¨¡å‹éƒ½æ— æ³•åŠ è½½ï¼Œå°†ä½¿ç”¨æ™ºèƒ½å›é€€åŠŸèƒ½")
            self.models_loaded = False
        
        return True  # Always return True so app can start
    
    def _load_sklearn_models(self):
        """å°è¯•åŠ è½½sklearnæ¨¡å‹"""
        try:
            print("ğŸ“¦ å°è¯•åŠ è½½sklearnæ¨¡å‹...")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(INTENT_MODEL_PATH):
                print(f"âŒ æ„å›¾åˆ†ç±»å™¨æ–‡ä»¶ä¸å­˜åœ¨: {INTENT_MODEL_PATH}")
                return False
                
            if not os.path.exists(LABEL_ENCODER_PATH):
                print(f"âŒ æ ‡ç­¾ç¼–ç å™¨æ–‡ä»¶ä¸å­˜åœ¨: {LABEL_ENCODER_PATH}")
                return False
            
            # å°è¯•åŠ è½½sklearnæ¨¡å‹
            print("ğŸ“„ åŠ è½½æ„å›¾åˆ†ç±»å™¨...")
            with open(INTENT_MODEL_PATH, "rb") as f:
                self.models["intent_classifier"] = pickle.load(f)
            print("âœ… æ„å›¾åˆ†ç±»å™¨åŠ è½½æˆåŠŸ")
            
            with open(LABEL_ENCODER_PATH, "rb") as f:
                self.models["label_encoder"] = pickle.load(f)
            print("âœ… æ ‡ç­¾ç¼–ç å™¨åŠ è½½æˆåŠŸ")
            
            self.sklearn_models_loaded = True
            print("ğŸ¯ sklearnæ¨¡å‹åŠ è½½å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ sklearnæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ æç¤º: è¿™é€šå¸¸æ˜¯sklearnç‰ˆæœ¬ä¸å…¼å®¹å¯¼è‡´çš„")
            self.sklearn_models_loaded = False
            return False
    
    def _load_transformer_models(self):
        """å°è¯•åŠ è½½transformeræ¨¡å‹"""
        try:
            print("ğŸ¤– å°è¯•åŠ è½½transformeræ¨¡å‹...")
            
            if not os.path.exists(NER_MODEL_PATH):
                print(f"âŒ NERæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {NER_MODEL_PATH}")
                return False
            
            # åŠ è½½NERæ¨¡å‹
            print("ğŸ”¤ åŠ è½½NERæ¨¡å‹...")
            self.models["ner_tokenizer"] = AutoTokenizer.from_pretrained(NER_MODEL_PATH)
            self.models["ner_model"] = AutoModelForTokenClassification.from_pretrained(NER_MODEL_PATH)
            self.models["ner_model"].eval()
            print("âœ… NERæ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # è·å–NERæ ‡ç­¾æ˜ å°„
            self.models["id2label"] = self.models["ner_model"].config.id2label
            print("âœ… NERæ ‡ç­¾æ˜ å°„è·å–æˆåŠŸ")
            
            # åŠ è½½BERTæ¨¡å‹ç”¨äºæå–åµŒå…¥
            print("ğŸ§  åŠ è½½BERTæ¨¡å‹...")
            self.models["bert_tokenizer"] = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)
            self.models["bert_model"] = AutoModel.from_pretrained(BERT_MODEL_NAME)
            print("âœ… BERTæ¨¡å‹åŠ è½½æˆåŠŸ")
            
            self.transformers_models_loaded = True
            print("ğŸ¯ transformeræ¨¡å‹åŠ è½½å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ transformeræ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.transformers_models_loaded = False
            return False
    
    def get_embedding(self, text):
        """ä»æ–‡æœ¬ä¸­æå–BERTåµŒå…¥å‘é‡"""
        if not self.transformers_models_loaded or "bert_tokenizer" not in self.models:
            # è¿”å›éšæœºåµŒå…¥ä½œä¸ºfallback
            print("âš ï¸ BERTæ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨éšæœºåµŒå…¥")
            return np.random.rand(1, 768)  # DistilBERTçš„åµŒå…¥ç»´åº¦
            
        try:
            tokenizer = self.models["bert_tokenizer"]
            model = self.models["bert_model"]
            
            inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
            
            with torch.no_grad():
                outputs = model(**inputs)
            
            # ä½¿ç”¨[CLS]æ ‡è®°è¡¨ç¤º
            embedding = outputs.last_hidden_state[:, 0, :].numpy()
            return embedding
        except Exception as e:
            print(f"BERTåµŒå…¥æå–å¤±è´¥: {e}")
            return np.random.rand(1, 768)
    
    def predict_intent(self, text):
        """é¢„æµ‹æ–‡æœ¬çš„æ„å›¾"""
        # æ™ºèƒ½æ„å›¾è¯†åˆ« - å³ä½¿æ²¡æœ‰æ¨¡å‹ä¹Ÿèƒ½å·¥ä½œ
        if self.sklearn_models_loaded:
            try:
                embedding = self.get_embedding(text)
                
                classifier = self.models["intent_classifier"]
                label_encoder = self.models["label_encoder"]
                
                # è·å–é¢„æµ‹æ¦‚ç‡
                probabilities = classifier.predict_proba(embedding)[0]
                
                # è·å–é¢„æµ‹ç±»åˆ«ç´¢å¼•
                prediction = classifier.predict(embedding)[0]
                
                # è·å–æ„å›¾åç§°
                intent = label_encoder.inverse_transform([prediction])[0]
                
                # è·å–ç½®ä¿¡åº¦ï¼ˆé¢„æµ‹ç±»åˆ«çš„æ¦‚ç‡ï¼‰
                confidence = float(probabilities[prediction])
                
                # è·å–æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡å¹¶æ’åº
                all_probs = []
                for i, prob in enumerate(probabilities):
                    intent_name = label_encoder.inverse_transform([i])[0]
                    all_probs.append({"name": intent_name, "confidence": float(prob)})
                
                all_probs.sort(key=lambda x: x["confidence"], reverse=True)
                
                return {
                    "name": intent,
                    "confidence": confidence,
                    "all_intents": all_probs[:3]  # åªè¿”å›å‰3ä¸ªæœ€å¯èƒ½çš„æ„å›¾
                }
            except Exception as e:
                print(f"æ¨¡å‹æ„å›¾é¢„æµ‹å‡ºé”™: {e}")
        
        # æ™ºèƒ½å›é€€ï¼šåŸºäºå…³é”®è¯çš„æ„å›¾è¯†åˆ«
        return self._smart_intent_fallback(text)
    
    def _smart_intent_fallback(self, text):
        """æ™ºèƒ½æ„å›¾è¯†åˆ«å›é€€æ–¹æ¡ˆ"""
        text_lower = text.lower()
        
        # å®šä¹‰å…³é”®è¯æ¨¡å¼
        add_keywords = ['schedule', 'book', 'add', 'create', 'set', 'plan', 'arrange', 'meeting', 'appointment']
        delete_keywords = ['cancel', 'delete', 'remove', 'clear']
        update_keywords = ['change', 'update', 'modify', 'move', 'reschedule']
        query_keywords = ['when', 'what', 'show', 'list', 'find', 'search', 'tell', 'check']
        
        # è®¡ç®—åŒ¹é…åˆ†æ•°
        add_score = sum(1 for keyword in add_keywords if keyword in text_lower)
        delete_score = sum(1 for keyword in delete_keywords if keyword in text_lower)
        update_score = sum(1 for keyword in update_keywords if keyword in text_lower)
        query_score = sum(1 for keyword in query_keywords if keyword in text_lower)
        
        # ç¡®å®šæœ€é«˜åˆ†æ•°çš„æ„å›¾
        scores = {
            'add': add_score,
            'delete': delete_score, 
            'update': update_score,
            'query': query_score
        }
        
        best_intent = max(scores, key=scores.get)
        confidence = min(0.9, max(0.6, scores[best_intent] * 0.2))  # åŸºäºåŒ¹é…æ•°é‡çš„ç½®ä¿¡åº¦
        
        # å¦‚æœæ²¡æœ‰æ˜ç¡®åŒ¹é…ï¼Œé»˜è®¤ä¸ºæ·»åŠ 
        if scores[best_intent] == 0:
            best_intent = 'add'
            confidence = 0.5
        
        all_intents = [{"name": intent, "confidence": score * 0.2} for intent, score in scores.items()]
        all_intents.sort(key=lambda x: x["confidence"], reverse=True)
        
        print(f"ğŸ’¡ æ™ºèƒ½å›é€€è¯†åˆ«æ„å›¾: {best_intent} (ç½®ä¿¡åº¦: {confidence:.2f})")
        
        return {
            "name": best_intent,
            "confidence": confidence,
            "all_intents": all_intents[:3]
        }
    
    def predict_ner(self, text):
        """é¢„æµ‹æ–‡æœ¬ä¸­çš„å‘½åå®ä½“"""
        if self.transformers_models_loaded:
            try:
                return self._transformer_ner(text)
            except Exception as e:
                print(f"transformer NERé¢„æµ‹å‡ºé”™: {e}")
        
        # æ™ºèƒ½å›é€€ï¼šåŸºäºè§„åˆ™çš„å®ä½“è¯†åˆ«
        return self._smart_ner_fallback(text)
    
    def _transformer_ner(self, text):
        """ä½¿ç”¨transformeræ¨¡å‹è¿›è¡ŒNER"""
        # åˆ†è¯
        words = text.split()
        
        ner_model = self.models["ner_model"]
        ner_tokenizer = self.models["ner_tokenizer"]
        id2label = self.models["id2label"]
        
        # ä½¿ç”¨æ¨¡å‹çš„åˆ†è¯å™¨å¤„ç†
        inputs = ner_tokenizer(words, truncation=True, is_split_into_words=True, return_tensors="pt")
        
        # é¢„æµ‹
        with torch.no_grad():
            outputs = ner_model(**inputs)
        
        # è·å–é¢„æµ‹æ ‡ç­¾
        predictions = torch.argmax(outputs.logits, dim=2).squeeze().tolist()
        
        # å°†é¢„æµ‹ä¸å•è¯å¯¹é½
        word_ids = inputs.word_ids()
        predicted_labels = []
        current_word = None
        
        for idx, word_id in enumerate(word_ids):
            if word_id is None:
                continue
            
            if word_id != current_word:
                current_word = word_id
                predicted_labels.append(id2label[predictions[idx]])
        
        # å°†ç»“æœæ ¼å¼åŒ–ä¸ºå®ä½“åˆ—è¡¨
        ner_results = list(zip(words, predicted_labels))
        entities = []
        current_entity = None
        
        for i, (word, label) in enumerate(ner_results):
            if label.startswith("B-"):
                if current_entity:
                    entities.append(current_entity)
                entity_type = label[2:]  # ç§»é™¤ "B-"
                current_entity = {"type": entity_type, "text": word, "start": i}
            elif label.startswith("I-") and current_entity:
                current_entity["text"] += " " + word
            elif label == "O":
                if current_entity:
                    entities.append(current_entity)
                    current_entity = None
        
        if current_entity:
            entities.append(current_entity)
        
        return entities
    
    def _smart_ner_fallback(self, text):
        """æ™ºèƒ½å®ä½“è¯†åˆ«å›é€€æ–¹æ¡ˆ"""
        import re
        
        words = text.split()
        entities = []
        
        # æ—¶é—´æ¨¡å¼
        time_patterns = [
            r'\b(\d{1,2}):(\d{2})\s*(am|pm|AM|PM)?\b',  # 2:30 PM
            r'\b(\d{1,2})\s*(am|pm|AM|PM)\b',           # 2 PM
            r'\b(at|from|to)\s+(\d{1,2}):?(\d{2})?\s*(am|pm|AM|PM)?\b'
        ]
        
        # æ—¥æœŸå…³é”®è¯
        date_words = ['today', 'tomorrow', 'yesterday', 'next week', 'monday', 'tuesday', 
                     'wednesday', 'thursday', 'friday', 'saturday', 'sunday']
        
        # åŠ¨ä½œå…³é”®è¯
        action_words = ['schedule', 'book', 'add', 'create', 'cancel', 'delete', 'update', 'modify']
        
        # æ£€æµ‹æ—¶é—´
        for pattern in time_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                entities.append({
                    "type": "TIME", 
                    "text": match.group(),
                    "start": match.start()
                })
        
        # æ£€æµ‹æ—¥æœŸè¯æ±‡
        text_lower = text.lower()
        for date_word in date_words:
            if date_word in text_lower:
                start_pos = text_lower.find(date_word)
                entities.append({
                    "type": "DATE",
                    "text": date_word,
                    "start": start_pos
                })
        
        # æ£€æµ‹åŠ¨ä½œè¯æ±‡
        for action_word in action_words:
            if action_word in text_lower:
                start_pos = text_lower.find(action_word)
                entities.append({
                    "type": "ACTION",
                    "text": action_word,
                    "start": start_pos
                })
        
        # æ£€æµ‹å¯èƒ½çš„æ ‡é¢˜ï¼ˆå¤§å†™å¼€å¤´çš„è¿ç»­è¯æ±‡ï¼‰
        words = text.split()
        for i, word in enumerate(words):
            if word[0].isupper() and word.lower() not in [w.lower() for w in action_words + date_words]:
                # å¯èƒ½æ˜¯äº‹ä»¶æ ‡é¢˜
                title_words = [word]
                j = i + 1
                while j < len(words) and (words[j][0].isupper() or words[j].lower() in ['with', 'and', 'for']):
                    title_words.append(words[j])
                    j += 1
                
                if len(title_words) >= 1:
                    entities.append({
                        "type": "TITLE",
                        "text": " ".join(title_words),
                        "start": i
                    })
        
        print(f"ğŸ’¡ æ™ºèƒ½å›é€€è¯†åˆ«å®ä½“: {len(entities)}ä¸ªå®ä½“")
        return entities